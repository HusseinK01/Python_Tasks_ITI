{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Descripition \n",
    "\n",
    "In 2012, URL shortening service Bitly partnered with the US government website USA.gov to provide a feed of anonymous data gathered from users who shorten links ending with .gov or .mil.\n",
    "\n",
    "The text file comes in JSON format and here are some keys and their description. They are only the most important ones for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|key| description |\n",
    "|---|-----------|\n",
    "| a|Denotes information about the web browser and operating system|\n",
    "| tz | time zone |\n",
    "| r | URL the user come from |\n",
    "| u | URL where the user headed to |\n",
    "| t | Timestamp when the user start using the website in UNIX format |\n",
    "| hc | Timestamp when user exit the website in UNIX format |\n",
    "| cy | City from which the request intiated |\n",
    "| ll | Longitude and Latitude |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell, I tried to provide some helper code for better understanding and clearer vision\n",
    "\n",
    "-**HINT**- Those lines of code may be not helping at all with your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11',\n",
       " 'c': 'US',\n",
       " 'nk': 1,\n",
       " 'tz': 'America/New_York',\n",
       " 'gr': 'MA',\n",
       " 'g': 'A6qOVH',\n",
       " 'h': 'wfLQtf',\n",
       " 'l': 'orofrog',\n",
       " 'al': 'en-US,en;q=0.8',\n",
       " 'hh': '1.usa.gov',\n",
       " 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',\n",
       " 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991',\n",
       " 't': 1333307030,\n",
       " 'hc': 1333307037,\n",
       " 'cy': 'Danvers',\n",
       " 'll': [42.576698, -70.954903]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will try to retrieve one instance of the file in a list of dictionaries\n",
    "import json\n",
    "records = [json.loads(line) for line in open('data/usa.gov_click_data_1.json')]\n",
    "# Print the first occurance\n",
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a script can transform the JSON files to a DataFrame and commit each file to a sparete CSV file in the target directory and consider the following:\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All CSV files must have the following columns\n",
    "- web_browser\n",
    "        The web browser that has requested the service\n",
    "- operating_sys\n",
    "        operating system that intiated this request\n",
    "- from_url\n",
    "\n",
    "        The main URL the user came from\n",
    "\n",
    "    **note**:\n",
    "\n",
    "    If the retrived URL was in a long format `http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf`\n",
    "\n",
    "     make it appear in the file in a short format like this `www.facebook.com`\n",
    "     \n",
    "    \n",
    "- to_url\n",
    "\n",
    "       The same applied like `to_url`\n",
    "   \n",
    "- city\n",
    "\n",
    "        The city from which the the request was sent\n",
    "    \n",
    "- longitude\n",
    "\n",
    "        The longitude where the request was sent\n",
    "- latitude\n",
    "\n",
    "        The latitude where the request was sent\n",
    "\n",
    "- time_zone\n",
    "        \n",
    "        The time zone that the city follow\n",
    "        \n",
    "- time_in\n",
    "\n",
    "        Time when the request started\n",
    "- time_out\n",
    "        \n",
    "        Time when the request is ended\n",
    "        \n",
    "        \n",
    "**NOTE** :\n",
    "\n",
    "Because that some instances of the file are incomplete, you may encouter some NaN values in your transforamtion. Make sure that the final dataframes have no NaNs at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Script itself must do the following before and after trasforamtion: \n",
    "    \n",
    "- One positional argument which is the directory path with that have the files.\n",
    "\n",
    "\n",
    "- One optional argument **-u**. If this argument is passed will maintain the UNIX format of timpe stamp and if not                passed the time stamps will be converted.\n",
    "\n",
    "\n",
    "- Check if the files have any dublicates in between **checksum** and print a messeage that indicate that.\n",
    "\n",
    "\n",
    "- Print a message after converting each file with the number of rows transformed and the path of this file\n",
    "\n",
    "\n",
    "- At the end of this script print the total excution time.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Data\\usa.gov_click_data_1.json' transformed. Rows transformed: 8\n",
      "Warning: Duplicate entries found in file 'Data\\usa.gov_click_data_2.json' based on checksum.\n",
      "File 'Data\\usa.gov_click_data_2.json' transformed. Rows transformed: 2587\n",
      "Warning: Duplicate entries found in file 'Data\\usa.gov_click_data_3.json' based on checksum.\n",
      "File 'Data\\usa.gov_click_data_3.json' transformed. Rows transformed: 2587\n",
      "Total execution time: 0.87 seconds\n",
      "Total rows transformed across all files: 5182\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "def extract_base_url(url):\n",
    "    if isinstance(url, str):\n",
    "        parsed_url = urlparse(url)\n",
    "        return parsed_url.netloc\n",
    "    else:\n",
    "        return \"Not Available\"\n",
    "\n",
    "def calculate_checksum(row):\n",
    "    # Convert pandas Series to dictionary\n",
    "    data_dict = row.to_dict()\n",
    "    return hashlib.md5(json.dumps(data_dict, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "def extract_browser_and_os(user_agent):\n",
    "    parts = user_agent.split('/')\n",
    "    if len(parts) >= 2:\n",
    "        if len(parts) >= 3 and parts[-2] != parts[1]:\n",
    "            web_browser = parts[-2].strip()\n",
    "        else:\n",
    "            web_browser = \"Not Available\"\n",
    "        os_parts = parts[1].split('(')\n",
    "        if len(os_parts) > 1:\n",
    "            operating_system = os_parts[1].split(')')[0].strip()\n",
    "        else:\n",
    "            operating_system = \"Not Available\"\n",
    "        return operating_system, web_browser\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def process_json_files(directory_path, unix_format=False):\n",
    "    start_time = time.time()\n",
    "    total_rows_transformed = 0\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over JSON files in the directory\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "                df = pd.DataFrame(data)\n",
    "            \n",
    "            # Create checksum column\n",
    "            df['checksum'] = df.apply(calculate_checksum, axis=1)\n",
    "            \n",
    "            # Check for duplicates based on checksum\n",
    "            if df.duplicated(subset='checksum').any():\n",
    "                print(f\"Warning: Duplicate entries found in file '{file_path}' based on checksum.\")\n",
    "            \n",
    "            \n",
    "            #\n",
    "# Check if columns 'kw' and '_heartbeat_' exist before dropping them\n",
    "            if 'kw' in df.columns and '_heartbeat_' in df.columns:\n",
    "                df.drop(columns=['kw', '_heartbeat_'], inplace=True)\n",
    "            # Drop rows with missing values\n",
    "            df.dropna(inplace=True)\n",
    "            \n",
    "            # Convert timestamps if needed\n",
    "            if not unix_format:\n",
    "                df['time_in'] = pd.to_datetime(df['time_in'], unit='s')\n",
    "                df['time_out'] = pd.to_datetime(df['time_out'], unit='s')\n",
    "            \n",
    "            # Count number of rows transformed\n",
    "            rows_transformed = len(df)\n",
    "            total_rows_transformed += rows_transformed\n",
    "            print(f\"File '{file_path}' transformed. Rows transformed: {rows_transformed}\")\n",
    "            \n",
    "            # Append the processed DataFrame to the list\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into one\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Apply function to extract base URL\n",
    "    combined_df['u'] = combined_df['u'].apply(extract_base_url)\n",
    "    combined_df['r'] = combined_df['r'].apply(extract_base_url)\n",
    "\n",
    "    # Rename columns\n",
    "    combined_df.rename(columns={'cy': 'city', 'll': 'latitude_longitude', 'tz': 'time_zone', \n",
    "                        't': 'time_in', 'hc': 'time_out', 'r': 'url_incoming', 'u': 'url_outgoing', 'c': 'country_code'}, inplace=True)\n",
    "\n",
    "    combined_df[['operating_system', 'web_browser']] = combined_df['a'].apply(lambda x: pd.Series(extract_browser_and_os(x)))\n",
    "\n",
    "    # List of columns to keep in the desired order\n",
    "    columns_to_keep = ['operating_system', 'web_browser', 'country_code', 'time_zone', 'url_incoming', 'url_outgoing', 'time_in', 'time_out', 'city', 'latitude_longitude', 'checksum']\n",
    "\n",
    "    #Keep only the specified columns in the desired order\n",
    "    combined_df = combined_df.loc[:, columns_to_keep]\n",
    "\n",
    "    # Write DataFrame to CSV\n",
    "    combined_df.to_csv('Output/combined_data.csv', index=False)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Total rows transformed across all files: {total_rows_transformed}\")\n",
    "\n",
    "# Example usage:\n",
    "directory_path = 'Data'\n",
    "unix_format = True  # Set to True to maintain UNIX format for timestamps\n",
    "\n",
    "process_json_files(directory_path, unix_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
